### Kube cluster build for:
# Debian 12
# K8s version 1.30
# docker 5:27.0.3-1
# containerd 1.7.18-1
# Calico (CNI/Networking)
# Ceph (Storage Solution)

### Server User Account (Non-root)
SVR_USER="dev"
WORKDIR="/home/$SVR_USER/dev-dir"

##############################################  the basics  ############################################

# Switch to root
su -

# Add Helm Key & Repo
curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list

##  Basic stuff  ##
apt update
apt install -y nala && nala upgrade
nala --install-completion bash
nala --show-completion bash
# Only keep the top 5 fastest mirrors
# sudo nala fetch 1,2,3,4,5

# Install Packages
nala install -y tree git firewalld htop gpg curl accountsservice neofetch sudo network-manager vim net-tools ca-certificates apt-transport-https


# Aliases
echo "alias ll='ls -la'" >> ~/.bashrc
echo "alias ll='ls -la'" >> /home/$SVR_USER/.bash_aliases
echo "alias k='kubectl'" >> /home/$SVR_USER/.bash_aliases

# Setup non-root user
/sbin/usermod -a -G sudo $SVR_USER
echo "$SVR_USER ALL=(ALL) ALL" >> /etc/sudoers

# Setup Network Manager
echo "managed=true" >> /etc/NetworkManager/NetworkManager.conf
systemctl restart NetworkManager

# Kill the swap space
swapoff -a
sed -i 's_/swap.img_#/swap.img_g' /etc/fstab
(crontab -l 2>/dev/null; echo "@reboot /sbin/swapoff -a") | crontab - || true

#########################################  SSH Setup  #####################################################

# ssh-keygen

# echo "192.168.1.111 svr1" >> /etc/hosts
# echo "192.168.1.112 svr2" >> /etc/hosts
# echo "192.168.1.113 svr3" >> /etc/hosts

# ssh-copy-id svr1
# ssh-copy-id svr2
# ssh-copy-id svr3

####################################  Containerd install  ########################################
## REF: https://docs.docker.com/engine/install/debian/

# Add Docker's official GPG key:
install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc
chmod a+r /etc/apt/keyrings/docker.asc

# Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# Update and install
nala upgrade -y
nala install -y containerd.io 
# Add non-root user to the docker group
usermod $SVR_USER -aG docker

# Apt hold required packages:
apt-mark hold containerd.io 

################################### Kube install ################################
# REF: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

# Update kernel for CRI/CNI
cat <<EOF | tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay 
sudo modprobe br_netfilter

cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

sysctl -p /etc/sysctl.d/k8s.conf

# Add Kube repo keys:
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# Add kube repo
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

nala upgrade -y
nala install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl

systemctl enable --now kubelet

############################# containerd config #############################
## COMMENT OUT DISABLED PLUGINS
sed -i 's_/disabled_plugins_#/disabled_plugins_g' /etc/containerd/config.toml
# Update containerd config.toml
containerd config default > /etc/containerd/config.toml

# sed for this:
# SystemdCgroup = true

############################## kubeadm config #############################
# kubeadm-config.yaml
mkdir $WORKDIR
cat <<EOF | tee $WORKDIR/kubeadm-config.yaml
kind: ClusterConfiguration
apiVersion: kubeadm.k8s.io/v1beta3
kubernetesVersion: v1.30.3
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
cgroupDriver: systemd
EOF

# restart containerd after new config.toml
systemctl restart containerd.service

############################## Kube Bootstrap ####################################
# REF: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/

kubeadm init --config ~/dev-dir/kubeadm-config.yaml --v 1 | tee /tmp/bootstrap_log.txt
grep -a1 "kubeadm join" /tmp/bootstrap_log.txt

## Optional if you want to setup your own cluster IP-space
### Kube boostrap ip range
#KUBE_CIDR="172.31.0.0/16"
#--pod-network-cidr=$KUBE_CIDR
# Without specifying the above, the default Cillium network space for pods will be 10.0.0.0/24

## Cleanup Commands
# kubeadm reset
# rm -rf ~/.kube/*
# iptables -F
# rm -rf /etc/cni/net.d
# rm -f /tmp/bootstrap_log.txt
# systemctl restart containerd
# systemctl stop kubelet

# Root User
export KUBECONFIG=/etc/kubernetes/admin.conf

# # Run as sudo $SVR_USER
# mkdir -p $HOME/.kube
# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
# chown $(id -u):$(id -g) $HOME/.kube/config

# Got these tokens back
#kubeadm join 192.168.242.130:6443 --token qjw9j3.wccprdgjbnolo56f \
#--discovery-token-ca-cert-hash sha256:7e02f57c38b57560b16b9472aa45f6b3092efc7eb9b408556b2eb91fe2da68ad

# Untaint your control-plane nodes to allow pods to schedule there
# REF: https://kubernetes.io/docs/reference/labels-annotations-taints/#node-role-kubernetes-io-control-plane-taint
kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule-

############################## Install Kube CNI (Calico) #########################
### REF: https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart

# Cilium CLI install
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64

if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}

# Cilium install
# Note: After running this command it could take a while 
# before Cilium reports as healthy
cilium install --version 1.16.0

################ Ceph pre-reqs ################################
# add a couple disks to your vm
# fdisk -l
# fdisk /dev/<new device name>
# g (add a new gpt)
# n (add a new partitioning table)
# w (write disk)

################## Rook-Ceph Install ##########################
#REF: https://rook.io/docs/rook/latest-release/Getting-Started/quickstart/#deploy-the-rook-operator
#REF: https://rook.io/docs/rook/latest-release/Getting-Started/example-configurations/#cluster-crd
#REF: https://rook.io/docs/rook/latest-release/CRDs/Block-Storage/ceph-block-pool-crd/#erasure-coded

# Git Clone Rook Ceph and Deploy basic Ceph cluster
git clone --single-branch --branch v1.14.9 https://github.com/rook/rook.git
cd rook/deploy/examples
kubectl create -f crds.yaml -f common.yaml -f operator.yaml
kubectl create -f cluster.yaml

### operator.yaml tweaks
# enable rook discovery daemon in the operator.yaml
# sed this: ROOK_ENABLE_DISCOVERY_DAEMON: "true"

### cluster.yaml tweaks
## Consider CASE logic to determine how many mon/mgr per cluster - determined by node count
#REF: https://medium.com/@satheesh.mohandass/installing-rook-ceph-for-persistent-storage-on-single-node-openshift-c1102a8ced40

# Custom ConfigMap for overrides below:
# ---
# kind: ConfigMap
# apiVersion: v1
# metadata:
#   name: rook-config-override
#   namespace: rook-ceph
# data:
#   config: |
#     [global]
#     osd_pool_default_size = 1
# ---

# cluster.yaml, enable host = true to use host IPs vs Cluster/LB IPs
# cluster yaml, add configMap for overrides
# cluster yaml, add mon = 1
# cluster yaml, add mgr = 1
# cluster yaml, add osdsPerDevice = 1

### Install Krew for Ceph/Kubectl plugin
(
  set -x; cd "$(mktemp -d)" &&
  OS="$(uname | tr '[:upper:]' '[:lower:]')" &&
  ARCH="$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\(arm\)\(64\)\?.*/\1\2/' -e 's/aarch64$/arm64/')" &&
  KREW="krew-${OS}_${ARCH}" &&
  curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz" &&
  tar zxvf "${KREW}.tar.gz" &&
  ./"${KREW}" install krew
)

export PATH="${KREW_ROOT:-$HOME/.krew}/bin:$PATH"

kubectl krew install rook-ceph

# You can now run 'kubectl rook-ceph ceph status'


### Ceph storage class example:
# apiVersion: ceph.rook.io/v1
# kind: CephBlockPool
# metadata:
#   name: replicapool
#   namespace: rook-ceph # namespace:cluster
# spec:
#   failureDomain: host
#   replicated:
#     size: 1
#     # Disallow setting pool with replica 1, this could lead to data loss without recovery.
#     # Make sure you're *ABSOLUTELY CERTAIN* that is what you want
#     requireSafeReplicaSize: false
#     # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
#     # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
#     #targetSizeRatio: .5
# ---
# apiVersion: storage.k8s.io/v1
# kind: StorageClass
# metadata:
#    name: rook-ceph-block
# # Change "rook-ceph" provisioner prefix to match the operator namespace if needed
# provisioner: rook-ceph.rbd.csi.ceph.com  # driver:namespace:operator
# parameters:
#     # clusterID is the namespace where the rook cluster is running
#     # If you change this namespace, also change the namespace below where the secret namespaces are defined
#     clusterID: rook-ceph # namespace:cluster

#     # If you want to use erasure coded pool with RBD, you need to create
#     # two pools. one erasure coded and one replicated.
#     # You need to specify the replicated pool here in the `pool` parameter, it is
#     # used for the metadata of the images.
#     # The erasure coded pool must be set as the `dataPool` parameter below.
#     #dataPool: ec-data-pool
#     pool: replicapool

#     # RBD image format. Defaults to "2".
#     imageFormat: "2"

#     # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.
#     imageFeatures: layering

#     # The secrets contain Ceph admin credentials. These are generated automatically by the operator
#     # in the same namespace as the cluster.
#     csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
#     csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph # namespace:cluster
#     csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
#     csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph # namespace:cluster
#     csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
#     csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # namespace:cluster
#     # Specify the filesystem type of the volume. If not specified, csi-provisioner
#     # will set default as `ext4`.
#     csi.storage.k8s.io/fstype: ext4
# # uncomment the following to use rbd-nbd as mounter on supported nodes
# #mounter: rbd-nbd
# allowVolumeExpansion: true
# reclaimPolicy: Delete

############################## Common issues #####################################
# - Found multiple CRI endpoints on the host. >> You need to check for docker, crio, and containerd services. Only one can be running at kubeadm "init" or "join"
# - Something about "invalid arguments from /proc* >> you need to make sure your config file has the right settings, you've done sysctl --system, and compelted a reboot.
#   - For more on this issue, check line 
# cat /etc/kubernetes/admin.conf
# kubeadm reset if you fuck up the master, do cleanup actions up top
# also run: rm -rf /etc/cni/net.d/
# flannel requires the "--pod-network-cidr=10.244.0.0/16"
# if you are too slow with the "join.sh" script, the token will expire. make a new one with:
# kubeadm token create --print-join-command

## Remove all Taints // if you want to schedule apps on the master node by default
# kubectl taint nodes elastic1 key-